{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7835977",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://colab.research.google.com/github/facebookresearch/dinov3/blob/main/dinov3_zero_shot_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "\n",
    "This notebook demonstrates how to perform zero-shot segmentation using DINOv3 features based on patch similarity. You can:\n",
    "\n",
    "1. **Upload an image** to segment\n",
    "2. **Click a seed point** on the image to define the region of interest\n",
    "3. **Extract DINOv3 patch features** and calculate cosine similarity\n",
    "4. **Adjust similarity threshold** interactively to refine the segmentation\n",
    "5. **Visualize results** with similarity heatmaps and segmentation overlays\n",
    "\n",
    "The approach leverages the rich semantic representations learned by DINOv3 to segment objects based on visual similarity to a user-selected seed region.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dd05cb",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0203e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install Pillow numpy matplotlib ipywidgets\n",
    "!pip install scikit-image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d9714",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.widgets import Button\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from google.colab import files\n",
    "import io\n",
    "import base64\n",
    "from skimage.transform import resize\n",
    "from skimage.filters import gaussian\n",
    "import subprocess\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a5b9e",
   "metadata": {},
   "source": [
    "## 2. Load DINOv3 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DINOV3_REPO_DIR = \"/content/dinov3\"\n",
    "MODEL_NAME = \"dinov3_vitl16\"\n",
    "PATCH_SIZE = 16\n",
    "FEATURE_DIM = 1024  # ViT-L feature dimension\n",
    "N_LAYERS = 24       # ViT-L has 24 layers\n",
    "\n",
    "print(\"Cloning DINOv3 repository...\")\n",
    "if not os.path.exists(DINOV3_REPO_DIR):\n",
    "    try:\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/facebookresearch/dinov3.git\", DINOV3_REPO_DIR], \n",
    "                      check=True, capture_output=True, text=True)\n",
    "        print(\"Repository cloned successfully!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to clone repository: {e}\")\n",
    "        print(\"Falling back to GitHub loading (may encounter rate limits)...\")\n",
    "        DINOV3_REPO_DIR = \"facebookresearch/dinov3\"\n",
    "\n",
    "print(\"Loading DINOv3 ViT-L/16 model...\")\n",
    "model = torch.hub.load(\n",
    "    repo_or_dir=DINOV3_REPO_DIR,\n",
    "    model=MODEL_NAME,\n",
    "    source=\"local\" if os.path.exists(DINOV3_REPO_DIR) else \"github\"\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(\"Model loaded successfully!\")\n",
    "print(f\"Model architecture: {MODEL_NAME}\")\n",
    "print(f\"Patch size: {PATCH_SIZE}x{PATCH_SIZE}\")\n",
    "print(f\"Feature dimension: {FEATURE_DIM}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f4fe18",
   "metadata": {},
   "source": [
    "## 3. Define Constants and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85c4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "IMAGE_SIZE = 768  # 768 = 16 * 48 patches\n",
    "\n",
    "def resize_to_patch_aligned(image, target_size=IMAGE_SIZE, patch_size=PATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Resize image to be aligned with patch grid while maintaining aspect ratio.\n",
    "    \"\"\"\n",
    "    if isinstance(image, Image.Image):\n",
    "        w, h = image.size\n",
    "        image = TF.to_tensor(image)\n",
    "    else:\n",
    "        _, h, w = image.shape\n",
    "    \n",
    "    aspect_ratio = w / h\n",
    "    if aspect_ratio > 1:  # Width > Height\n",
    "        target_w = target_size\n",
    "        target_h = int(target_size / aspect_ratio)\n",
    "    else:  # Height >= Width\n",
    "        target_h = target_size\n",
    "        target_w = int(target_size * aspect_ratio)\n",
    "    \n",
    "    target_h = (target_h // patch_size) * patch_size\n",
    "    target_w = (target_w // patch_size) * patch_size\n",
    "    \n",
    "    target_h = max(target_h, patch_size)\n",
    "    target_w = max(target_w, patch_size)\n",
    "    \n",
    "    return TF.resize(image, (target_h, target_w))\n",
    "\n",
    "def normalize_image(image):\n",
    "    \"\"\"\n",
    "    Normalize image with ImageNet statistics.\n",
    "    \"\"\"\n",
    "    return TF.normalize(image, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
    "\n",
    "def pixel_to_patch_coords(pixel_x, pixel_y, patch_size=PATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Convert pixel coordinates to patch coordinates.\n",
    "    \"\"\"\n",
    "    patch_x = pixel_x // patch_size\n",
    "    patch_y = pixel_y // patch_size\n",
    "    return int(patch_x), int(patch_y)\n",
    "\n",
    "def extract_features(model, image_tensor, n_layers=N_LAYERS):\n",
    "    \"\"\"\n",
    "    Extract dense features from DINOv3 model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu', dtype=torch.float32):\n",
    "            features = model.get_intermediate_layers(\n",
    "                image_tensor.unsqueeze(0).to(device),\n",
    "                n=range(n_layers),\n",
    "                reshape=True,\n",
    "                norm=True\n",
    "            )\n",
    "            return features[-1].squeeze(0)  # Remove batch dimension\n",
    "\n",
    "def calculate_cosine_similarity(features, seed_feature):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between seed feature and all patch features.\n",
    "    \n",
    "    Args:\n",
    "        features: (D, H, W) - patch features\n",
    "        seed_feature: (D,) - seed patch feature vector\n",
    "    \n",
    "    Returns:\n",
    "        similarity_map: (H, W) - cosine similarity scores\n",
    "    \"\"\"\n",
    "    D, H, W = features.shape\n",
    "    \n",
    "    features_flat = features.view(D, -1)\n",
    "    \n",
    "    features_norm = F.normalize(features_flat, dim=0)  # Normalize each spatial location\n",
    "    seed_norm = F.normalize(seed_feature.unsqueeze(0), dim=1)  # (1, D)\n",
    "    \n",
    "    similarity_flat = torch.mm(seed_norm, features_norm)  # (1, H*W)\n",
    "    \n",
    "    similarity_map = similarity_flat.view(H, W)\n",
    "    \n",
    "    return similarity_map\n",
    "\n",
    "print(\"Helper functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef866530",
   "metadata": {},
   "source": [
    "## 4. Image Upload and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ed79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Please upload an image file:\")\n",
    "uploaded = files.upload()\n",
    "\n",
    "if uploaded:\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    image_data = uploaded[filename]\n",
    "    \n",
    "    original_image = Image.open(io.BytesIO(image_data)).convert('RGB')\n",
    "    print(f\"Original image size: {original_image.size}\")\n",
    "    \n",
    "    processed_image = resize_to_patch_aligned(original_image)\n",
    "    normalized_image = normalize_image(processed_image)\n",
    "    \n",
    "    _, height, width = processed_image.shape\n",
    "    patch_h = height // PATCH_SIZE\n",
    "    patch_w = width // PATCH_SIZE\n",
    "    \n",
    "    print(f\"Processed image size: {width}x{height}\")\n",
    "    print(f\"Patch grid: {patch_w}x{patch_h} patches\")\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(processed_image.permute(1, 2, 0))\n",
    "    plt.title(f\"Uploaded Image (resized to {width}x{height})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No image uploaded. Please run this cell again and upload an image.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61218d9",
   "metadata": {},
   "source": [
    "## 5. Extract DINOv3 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97805ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'normalized_image' in locals():\n",
    "    print(\"Extracting DINOv3 features...\")\n",
    "    \n",
    "    features = extract_features(model, normalized_image)\n",
    "    \n",
    "    print(f\"Feature shape: {features.shape}\")\n",
    "    print(f\"Feature dimension: {features.shape[0]}\")\n",
    "    print(f\"Spatial resolution: {features.shape[1]}x{features.shape[2]}\")\n",
    "    \n",
    "    features_cpu = features.cpu()\n",
    "    \n",
    "    print(\"Feature extraction completed!\")\n",
    "else:\n",
    "    print(\"Please upload an image first in the previous cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c0f74a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Click on the image below to select a seed point. The red circle will show your selected location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5b81a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'processed_image' in locals() and 'features_cpu' in locals():\n",
    "    seed_coords = None\n",
    "    seed_feature = None\n",
    "    \n",
    "    def on_click(event):\n",
    "        global seed_coords, seed_feature\n",
    "        \n",
    "        if event.inaxes is None:\n",
    "            return\n",
    "        \n",
    "        x, y = int(event.xdata), int(event.ydata)\n",
    "        \n",
    "        patch_x, patch_y = pixel_to_patch_coords(x, y)\n",
    "        \n",
    "        patch_x = max(0, min(patch_x, patch_w - 1))\n",
    "        patch_y = max(0, min(patch_y, patch_h - 1))\n",
    "        \n",
    "        seed_coords = (patch_x, patch_y)\n",
    "        \n",
    "        seed_feature = features_cpu[:, patch_y, patch_x]  # (D,)\n",
    "        \n",
    "        ax.clear()\n",
    "        ax.imshow(processed_image.permute(1, 2, 0))\n",
    "        \n",
    "        circle = patches.Circle((x, y), radius=8, color='red', fill=True, alpha=0.8)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        patch_rect = patches.Rectangle(\n",
    "            (patch_x * PATCH_SIZE, patch_y * PATCH_SIZE),\n",
    "            PATCH_SIZE, PATCH_SIZE,\n",
    "            linewidth=2, edgecolor='red', facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(patch_rect)\n",
    "        \n",
    "        ax.set_title(f\"Seed Point: Pixel ({x}, {y}) → Patch ({patch_x}, {patch_y})\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.draw()\n",
    "        \n",
    "        print(f\"Selected seed point: Pixel ({x}, {y}) → Patch ({patch_x}, {patch_y})\")\n",
    "        print(f\"Seed feature shape: {seed_feature.shape}\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    ax.imshow(processed_image.permute(1, 2, 0))\n",
    "    ax.set_title(\"Click on the image to select a seed point\")\n",
    "    ax.axis('off')\n",
    "    \n",
    "    cid = fig.canvas.mpl_connect('button_press_event', on_click)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nInstructions:\")\n",
    "    print(\"1. Click anywhere on the image to select a seed point\")\n",
    "    print(\"2. The red circle shows your click location\")\n",
    "    print(\"3. The red rectangle shows the corresponding 16x16 patch\")\n",
    "    print(\"4. After selecting, run the next cell to calculate similarity\")\n",
    "    \n",
    "else:\n",
    "    print(\"Please upload an image and extract features first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b3e19e",
   "metadata": {},
   "source": [
    "## 7. Calculate Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f39e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'seed_feature' in locals() and seed_feature is not None:\n",
    "    print(\"Calculating cosine similarity between seed patch and all other patches...\")\n",
    "    \n",
    "    similarity_map = calculate_cosine_similarity(features_cpu, seed_feature)\n",
    "    \n",
    "    print(f\"Similarity map shape: {similarity_map.shape}\")\n",
    "    print(f\"Similarity range: [{similarity_map.min():.3f}, {similarity_map.max():.3f}]\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    axes[0].imshow(processed_image.permute(1, 2, 0))\n",
    "    axes[0].set_title(\"Original Image\")\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    if seed_coords:\n",
    "        seed_x, seed_y = seed_coords\n",
    "        circle = patches.Circle(\n",
    "            (seed_x * PATCH_SIZE + PATCH_SIZE//2, seed_y * PATCH_SIZE + PATCH_SIZE//2),\n",
    "            radius=8, color='red', fill=True, alpha=0.8\n",
    "        )\n",
    "        axes[0].add_patch(circle)\n",
    "    \n",
    "    im1 = axes[1].imshow(similarity_map, cmap='viridis', vmin=-1, vmax=1)\n",
    "    axes[1].set_title(f\"Similarity Map ({patch_h}x{patch_w})\")\n",
    "    axes[1].axis('off')\n",
    "    plt.colorbar(im1, ax=axes[1], shrink=0.8)\n",
    "    \n",
    "    similarity_upsampled = torch.nn.functional.interpolate(\n",
    "        similarity_map.unsqueeze(0).unsqueeze(0),\n",
    "        size=(height, width),\n",
    "        mode='bilinear',\n",
    "        align_corners=False\n",
    "    ).squeeze()\n",
    "    \n",
    "    im2 = axes[2].imshow(similarity_upsampled, cmap='viridis', vmin=-1, vmax=1)\n",
    "    axes[2].set_title(f\"Upsampled Similarity ({height}x{width})\")\n",
    "    axes[2].axis('off')\n",
    "    plt.colorbar(im2, ax=axes[2], shrink=0.8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nSimilarity calculation completed!\")\n",
    "    print(\"Proceed to the next cell for interactive threshold adjustment.\")\n",
    "    \n",
    "else:\n",
    "    print(\"Please select a seed point first by clicking on the image in the previous cell.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfd8034",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Use the slider below to adjust the similarity threshold and see the resulting segmentation mask in real-time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfbab63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'similarity_map' in locals():\n",
    "    output = widgets.Output()\n",
    "    \n",
    "    def update_segmentation(threshold):\n",
    "        with output:\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "            binary_mask = (similarity_map >= threshold).float()\n",
    "            \n",
    "            mask_upsampled = torch.nn.functional.interpolate(\n",
    "                binary_mask.unsqueeze(0).unsqueeze(0),\n",
    "                size=(height, width),\n",
    "                mode='nearest'\n",
    "            ).squeeze()\n",
    "            \n",
    "            mask_smooth = gaussian(mask_upsampled.numpy(), sigma=1.0)\n",
    "            \n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "            \n",
    "            axes[0, 0].imshow(processed_image.permute(1, 2, 0))\n",
    "            axes[0, 0].set_title(\"Original Image\")\n",
    "            axes[0, 0].axis('off')\n",
    "            \n",
    "            if seed_coords:\n",
    "                seed_x, seed_y = seed_coords\n",
    "                circle = patches.Circle(\n",
    "                    (seed_x * PATCH_SIZE + PATCH_SIZE//2, seed_y * PATCH_SIZE + PATCH_SIZE//2),\n",
    "                    radius=8, color='red', fill=True, alpha=0.8\n",
    "                )\n",
    "                axes[0, 0].add_patch(circle)\n",
    "            \n",
    "            im1 = axes[0, 1].imshow(similarity_upsampled, cmap='viridis', vmin=-1, vmax=1)\n",
    "            axes[0, 1].axhline(y=height//2, color='white', linestyle='--', alpha=0.5)\n",
    "            axes[0, 1].axvline(x=width//2, color='white', linestyle='--', alpha=0.5)\n",
    "            axes[0, 1].set_title(f\"Similarity Map (Threshold: {threshold:.2f})\")\n",
    "            axes[0, 1].axis('off')\n",
    "            \n",
    "            cbar1 = plt.colorbar(im1, ax=axes[0, 1], shrink=0.8)\n",
    "            cbar1.ax.axhline(y=threshold, color='red', linewidth=2)\n",
    "            \n",
    "            axes[0, 2].imshow(mask_smooth, cmap='gray', vmin=0, vmax=1)\n",
    "            axes[0, 2].set_title(\"Segmentation Mask\")\n",
    "            axes[0, 2].axis('off')\n",
    "            \n",
    "            axes[1, 0].imshow(processed_image.permute(1, 2, 0))\n",
    "            axes[1, 0].imshow(similarity_upsampled, cmap='viridis', alpha=0.4, vmin=-1, vmax=1)\n",
    "            axes[1, 0].set_title(\"Image + Similarity Overlay\")\n",
    "            axes[1, 0].axis('off')\n",
    "            \n",
    "            axes[1, 1].imshow(processed_image.permute(1, 2, 0))\n",
    "            axes[1, 1].imshow(mask_smooth, cmap='Reds', alpha=0.5, vmin=0, vmax=1)\n",
    "            axes[1, 1].set_title(\"Image + Mask Overlay\")\n",
    "            axes[1, 1].axis('off')\n",
    "            \n",
    "            image_np = processed_image.permute(1, 2, 0).numpy()\n",
    "            segmented = image_np * mask_smooth[:, :, np.newaxis]\n",
    "            axes[1, 2].imshow(segmented)\n",
    "            axes[1, 2].set_title(\"Segmented Region\")\n",
    "            axes[1, 2].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            mask_area = mask_upsampled.sum().item()\n",
    "            total_area = mask_upsampled.numel()\n",
    "            coverage = mask_area / total_area * 100\n",
    "            \n",
    "            print(f\"Threshold: {threshold:.3f}\")\n",
    "            print(f\"Mask coverage: {coverage:.1f}% ({int(mask_area)} / {total_area} pixels)\")\n",
    "            print(f\"Similarity range in mask: [{similarity_upsampled[mask_upsampled > 0].min():.3f}, {similarity_upsampled[mask_upsampled > 0].max():.3f}]\")\n",
    "    \n",
    "    threshold_slider = widgets.FloatSlider(\n",
    "        value=0.5,\n",
    "        min=-1.0,\n",
    "        max=1.0,\n",
    "        step=0.01,\n",
    "        description='Threshold:',\n",
    "        style={'description_width': 'initial'},\n",
    "        layout=widgets.Layout(width='500px')\n",
    "    )\n",
    "    \n",
    "    interactive_widget = widgets.interactive(update_segmentation, threshold=threshold_slider)\n",
    "    \n",
    "    display(interactive_widget)\n",
    "    display(output)\n",
    "    \n",
    "    print(\"\\nInstructions:\")\n",
    "    print(\"• Move the threshold slider to adjust the segmentation\")\n",
    "    print(\"• Higher threshold = more selective (smaller regions)\")\n",
    "    print(\"• Lower threshold = less selective (larger regions)\")\n",
    "    print(\"• The red line on the colorbar shows the current threshold\")\n",
    "    \n",
    "else:\n",
    "    print(\"Please complete the similarity calculation first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9c83a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Save the segmentation results to files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'similarity_map' in locals():\n",
    "    current_threshold = threshold_slider.value if 'threshold_slider' in locals() else 0.5\n",
    "    \n",
    "    binary_mask = (similarity_map >= current_threshold).float()\n",
    "    mask_upsampled = torch.nn.functional.interpolate(\n",
    "        binary_mask.unsqueeze(0).unsqueeze(0),\n",
    "        size=(height, width),\n",
    "        mode='nearest'\n",
    "    ).squeeze()\n",
    "    \n",
    "    original_pil = TF.to_pil_image(processed_image)\n",
    "    mask_pil = TF.to_pil_image(mask_upsampled)\n",
    "    similarity_pil = TF.to_pil_image((similarity_upsampled + 1) / 2)  # Normalize to [0,1]\n",
    "    \n",
    "    overlay = Image.blend(original_pil.convert('RGBA'), \n",
    "                         Image.new('RGBA', original_pil.size, (255, 0, 0, 128)), \n",
    "                         0.3)\n",
    "    overlay.paste(original_pil, mask=mask_pil.convert('L'))\n",
    "    \n",
    "    original_pil.save('dinov3_original.png')\n",
    "    mask_pil.save('dinov3_mask.png')\n",
    "    similarity_pil.save('dinov3_similarity.png')\n",
    "    overlay.save('dinov3_overlay.png')\n",
    "    \n",
    "    print(f\"Results saved with threshold {current_threshold:.3f}:\")\n",
    "    print(\"• dinov3_original.png - Original processed image\")\n",
    "    print(\"• dinov3_mask.png - Binary segmentation mask\")\n",
    "    print(\"• dinov3_similarity.png - Similarity heatmap\")\n",
    "    print(\"• dinov3_overlay.png - Image with mask overlay\")\n",
    "    \n",
    "    files.download('dinov3_original.png')\n",
    "    files.download('dinov3_mask.png')\n",
    "    files.download('dinov3_similarity.png')\n",
    "    files.download('dinov3_overlay.png')\n",
    "    \n",
    "else:\n",
    "    print(\"No segmentation results to export. Please complete the workflow first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36148260",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This notebook demonstrated DINOv3-based zero-shot segmentation using patch feature similarity. Key points:\n",
    "\n",
    "1. **Loaded DINOv3 ViT-L/16** model with 1024-dimensional features\n",
    "2. **Interactive seed point selection** by clicking on images\n",
    "3. **Cosine similarity calculation** between patch features\n",
    "4. **Real-time threshold adjustment** with interactive slider\n",
    "5. **Multi-view visualization** of results\n",
    "\n",
    "- **Zero-shot**: No training required, works on any image\n",
    "- **Interactive**: Real-time feedback and adjustment\n",
    "- **Semantic**: Leverages DINOv3's rich visual representations\n",
    "- **Flexible**: Adjustable similarity thresholds\n",
    "\n",
    "- **Click on distinctive regions** with clear visual features\n",
    "- **Adjust threshold gradually** to find optimal segmentation\n",
    "- **Try different seed points** for the same object\n",
    "- **Higher thresholds** for more precise, smaller regions\n",
    "- **Lower thresholds** for larger, more inclusive regions\n",
    "\n",
    "- Multi-scale feature aggregation\n",
    "- Multiple seed points\n",
    "- Post-processing with CRF or GrabCut\n",
    "- Integration with object detection\n",
    "\n",
    "Feel free to experiment with different images and thresholds to explore the capabilities of DINOv3 features!\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
